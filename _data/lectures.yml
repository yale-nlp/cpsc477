- date: Tue 01/13/26
  lecturer:
    - Arman
  title:
    - Course Introduction
    - Logistics
  slides: https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EXwUUUhp8_FMg6GENBDX40cBUptr9lkWFTCapjQOsQk3VA?e=eSAzNY
  readings:
  optional:
  logistics:

- date: Thu 01/15/26
  lecturer:
  title:
    - Word embeddings and vector semantics
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EVn_42KRLbZCtI0ALmPi11ABUp_-Rsxfw-zdMxtZze887w?e=Cr7VL9
  readings:
    - Jurafsky & Martin Chapter 6
  optional:
  logistics:

- date: Tue 01/20/26
  lecturer:
  title:
    - Word embeddings and vector semantics (cont.)
    - Sparse representations
    - Dense representations
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ERYhVcO2wThCij2SS4QvrZoBf3uJXnqnpGhma_2QyWNQiA?e=pcgmGy
  readings:
    - Jurafsky & Martin Chapter 6
  optional:
    - Distributed Representation of Words and Phrases and their Compositionality (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">[link]</a>
    - Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank">[link]</a>
    - Word2vec Explained- deriving Mikolov et al.'s negative-sampling word-embedding method (Goldberg and Levy, 2014) <a href="https://arxiv.org/pdf/1402.3722.pdf" target="_blank">[link]</a>
  logistics:

- date: Thu 01/22/26
  lecturer:
  title:
    - Deriving the gradient of Word2vec
    - Evaluation of word embeddings
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EbzzWnTtbqxFoRQz5NDU4RQBar1GpnrcQuobMLNwiljwaQ?e=MD5nHZ
  annotated:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EdE6CsTv9BFPulS6tlAmb8ABFftomH_-1s2aoaqqFiaqCA?e=YXixeM
  readings:
    - Jurafsky & Martin Chapter 6
    - Distributed Representation of Words and Phrases and their Compositionality (Mikolov et al., 2013) <a href="https://arxiv.org/pdf/1310.4546.pdf" target="_blank">[link]</a>
  optional:
  logistics: HW1 out

- date: Thu 01/29/26
  lecturer:
  title:
    - N-Gram Language Models
    - Smoothing
    - Evaluation of Language Models
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ETSbGs3eQChEhDZBAb65JcABk22O4HbTHkRPdXYCca0c4Q?e=peWcBf
  readings:
    - Jurafsky & Martin Chapter 7
  optional:
  logistics:

- date: Fri 01/30/26
  lecturer:
    - Arman
  title:
    - Neural network basics
    - Autograd
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EWBDVm9WpaVPs4VimgD5uxQB5WkYtAJuumJ9XfGIxBuvPg?e=HhnhC6
  readings:
    - The Matrix Calculus You Need For Deep Learning (Terence Parr and Jeremy Howard) <a href="https://arxiv.org/pdf/1802.01528.pdf" target="_blank">[link]</a>
    - Little book of deep learning (Fran√ßois Fleuret) - Ch 3, 4
  optional:
  logistics:

- date: Tue 02/03/26
  lecturer:
    - Arman
  title:
    - Auto Grad
    - Building blocks of Deep Learning for NLP
    - CNNs
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EboO6OJwKV1OjGndZnaamNUB_5cZkmgvxoi8Tf5g2N3Kaw?e=Yw4qzo
  readings:
    - Goldberg Chapter 9
  optional:
  logistics:

- date: Thu 02/05/26
  lecturer:
    - Arman
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ET1MdP7nmdZIpw5wHVY6c4UBmseEI4E_440BQprXBFZmPQ?e=kHxgHP
  title:
    - CNNs (contd.)
    - RNNs
    - Task specific neural network architectures
    - Machine translation
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ET1MdP7nmdZIpw5wHVY6c4UBmseEI4E_440BQprXBFZmPQ?e=kHxgHP
  readings:
    - Understanding LSTM Networks (Christopher Olah) <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank">[link]</a>
    - Eisenstein, Chapter 18
  optional:
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
  logistics:

- date: Tue 02/10/26
  lecturer:
  title:
    - RNNs (contd.)
    - Training sequence models
    - Machine translation (contd.)
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EV2Z-ffxQSBBtFNL-nIXxcUBuMG75pXsiq1nmyuNKtJvgA?e=dzDvuM
  readings:
    - Statistical Machine Translation (Koehn) <a href="https://www.statmt.org/book/" target="_blank">[link]</a>
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
    - Learning to Align and Translate with Attention (Bahdanau et al., 2015) <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">[link]</a>
    - Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
  logistics: Project teams due on 02/08

- date: Thu 02/12/26
  lecturer:
  title:
    - Attention
    - Transformers
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ET8CJC5rp2RKrXAu_q5IjI0BUZ8w_VxGFukAyVaVEXzJrg?e=d7ALEQ
  annotated:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EYIudrCDWFZNqIUkddqwi7UBRE0mZnssTqCnjYapuonbSQ?e=TsvbQJ
  readings:
    - Neural Machine Translation and Sequence-to-sequence Models- A Tutorial (Graham Neubig) <a href="https://arxiv.org/pdf/1703.01619.pdf" target="_blank">[link]</a>
    - Learning to Align and Translate with Attention (Bahdanau et al., 2015) <a href="https://arxiv.org/pdf/1409.0473.pdf" target="_blank">[link]</a>
    - Luong et al. (2015) Effective Approaches to Attention-based Neural Machine Translation <a href="https://arxiv.org/pdf/1508.04025.pdf" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
  optional:

- date: Tue 02/17/26
  lecturer:
    - Arman
  title:
    - Transformers (contd.)
    - Language modeling with Transformers
    - Transfer Learning
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EQ7n94Gc7PpAkd462VJo3uIBFGo3VSFxlzXt83OAKba_9A?e=HmthNf
  readings:
    - Illustrated Transformer <a href="http://jalammar.github.io/illustrated-transformer/" target="_blank">[link]</a>
    - Attention is All You Need (Vaswani et al., 2017) <a href="https://arxiv.org/pdf/1706.03762.pdf" target="_blank">[link]</a>
    - The Annotated Transformer (Harvard NLP) <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html" target="_blank">[link]</a>
    - GPT-2 (Radford et al., 2019) <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">[link]</a>
  optional:
  logistics: HW 1 due / HW 2 out

- date: Thu 02/19/26
  lecturer:
    - Arman
  title:
    - Transfer Learning (contd.)
    - Objective functions for pre-training
    - Encoder-decoder pretrained models
    - Architecture and pretraining objectives
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EZgliHqYSG1MrqM8UnbKw6YBNMfTwKwFzZFStln2_EbtUQ?e=1GRIfB
  readings:
    - The Illustrated BERT, ELMo, and co. (Jay Alammar) <a href="http://jalammar.github.io/illustrated-bert/" target="_blank">[link]</a>
    - BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018) <a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank">[link]</a>
    - GPT-2 (Radford et al., 2019) <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank">[link]</a>
    - T5- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al., 2020) <a href="https://arxiv.org/pdf/1910.10683.pdf" target="_blank">[link]</a>
    - BART- Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension (Lewis et al., 2019) <a href="https://arxiv.org/pdf/1910.13461.pdf" target="_blank">[link]</a>
    - What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization? (Wang et al, 2022) <a href="https://arxiv.org/abs/2204.05832" target="_blank">[link]</a>
  optional:
  logistics:

- date: Tue 02/24/26
  lecturer:
    - Arman
  title:
    - Decoding and generation
    - Large language models and impact of scale
    - In-context learning and prompting
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/ERM7prxa__JHr06GrtwOCBYBPwcIZV8UrMaA4Cdu3BFpjg?e=81LHwi
  readings:
    - The Curious Case of Neural Text Degeneration (Holtzman et al., 2019) <a href="https://arxiv.org/pdf/1904.09751.pdf" target="_blank">[link]</a>
    - How to generate text- using different decoding methods for language generation with Transformers <a href="https://huggingface.co/blog/how-to-generate" target="_blank">[link]</a>
    - Scaling Laws for Neural Language Models (Kaplan et al., 2020) <a href="https://arxiv.org/pdf/2001.08361.pdf" target="_blank">[link]</a>
    - Training Compute-Optimal Large Language Models (Hoffmann et al., 2022) <a href="https://arxiv.org/pdf/2203.15556.pdf" target="_blank">[link]</a>
    - GPT3 paper - Language Models are Few-Shot Learners (Brown et al., 2020) <a href="https://arxiv.org/pdf/2005.14165.pdf" target="_blank">[link]</a>

- date: Thu 02/26/26
  lecturer:
    - Arman
  title: >
    <strong> Midterm Exam 1 </strong>

- date: Tue 03/03/26
  title:
    - "Guest Lecture 1: TBD"
  slides:
  readings:
  optional:
  logistics: HW 2 due

- date: Thu 03/05/26
  lecturer:
    - Arman
  title:
    - Post-training
    - Supervised Finetuning
    - Instruction Following
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EUDYKY_XkMdPlLpdJsJ9AvwBMP2uc0pqVEiKJf4VES0F0g?e=XDeJWa
  readings:
    - Multitask Prompted Training Enables Zero-Shot Task Generalization (Sanh et al., 2021) <a href="https://arxiv.org/abs/2110.08207" target="_blank">[link]</a>
    - Scaling Instruction-Finetuned Language Models (Chung et al., 2022) <a href="https://arxiv.org/abs/2210.11416" target="_blank">[link]</a>
    - Are Emergent Abilities of Large Language Models a Mirage? (Sha et al., 2023) <a href="https://arxiv.org/pdf/2304.15004.pdf" target="_blank">[link]</a>
    - Emergent Abilities of Large Language Models (Wei et al., 2022) <a href="https://arxiv.org/abs/2206.07682" target="_blank">[link]</a>
  optional:
  logistics: HW2 due on 03/05 <br/><br/> Project proposals due 03/07;

- date: 03/06/26 - 03/22/26
  title: >
    <strong> Spring recess - No classes </strong>

- date: Tue 03/24/26
  title:
    - "Guest Lecture 2: TBD"
  slides:
  readings:
  optional:
  logistics:

- date: Thu 03/26/26
  title:
    - Post-training
    - Reinforcement learning from Human Feedback
    - Alignment
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/Eao1PnDqk_RGpGpZqpg7WqcB5VJeU29cw1iCm5z91Lmlug?e=3b1Hfp
  readings:
    - Training language models to follow instructions with human feedback (Ouyang et al., 2022) <a href="https://arxiv.org/abs/2203.02155" target="_blank">[link]</a>
    - Fine-Tuning Language Models from Human Preferences (Ziegler et al., 2019) <a href="https://arxiv.org/abs/1909.08593" target="_blank">[link]</a>
    - Direct Preference Optimization- Your Language Model is Secretly a Reward Model (Rafailov et al., 2023) <a href="https://arxiv.org/abs/2305.18290" target="_blank">[link]</a>
    - RLAIF- Scaling Reinforcement Learning from Human Feedback with AI Feedback (Lee et al., 2023) <a href="https://arxiv.org/abs/2309.00267" target="_blank">[link]</a>
  optional:
  logistics:

- date: Tue 03/31/26
  title:
    - Post-training (contd...)
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EQgLrSEIxwlHq7DPGlQ95osBSw4pQrBnzIbBFwVW7payGQ?e=pzFaTv
  readings:
  optional:
  logistics: HW 3 out

- date: Thu 04/02/26
  title:
    - "Guest Lecture 3: TBD"
  slides:
  readings:
  optional:
  logistics:

- date: Tue 04/07/26
  lecturer:
  title:
    - Retrieval Augmented Generation (RAG)
  slides:
    - https://yaleedu-my.sharepoint.com/:b:/g/personal/arman_cohan_yale_edu/EZvLa2IIKLhJipRCZVgOLREB8MDQt8Qwo_T0mKc_GTBmNg?e=lIoZEU
  readings:
  optional:
  logistics:

- date: Thu 04/09/26
  lecturer:
    - Arman
  title: >
    <strong> Midterm Exam 2 </strong>

- date: Tue 04/14/26
  title:
    - "Guest Lecture 4: TBD"
  slides:
  readings:
  optional:
  logistics:

- date: Thu 04/16/26
  lecturer:
  title: RAG continued, Intro to Agent-based systems
  slides:
  readings:
  optional:
  logistics:

- date: Tue 04/21/26
  lecturer:
  title:
    -  Project presentations session 1
  slides:
  readings:
  optional:
  logistics: Final project presentations

- date: Thu 04/23/26
  lecturer:
  title: Project presentations session 2
  slides:
  readings:
  optional:
  logistics: Final project presentations, HW 3 due on 4/27, <br/> Final project reports due on 4/30
